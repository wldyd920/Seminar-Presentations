제가 오늘 발표 드릴 내용은 AutoML 분야에서 유명한 DARTS를 가져와봤습니다.

먼저 논문 내용을 요약해서 말씀 드린 후에,
DARTS에서 가지고 있는 취약점에 대해 말씀 드리고,
제 아이디어에 대해서 논의 해보고자 합니다.

이 논문은 NAS를 강화학습이나 진화 알고리즘으로 접근한 다른 논문들과 달리 시퀀셜한 도메인 내에서 그래프 토폴로지를 사용해서 미분 가능하게 만들었고, 이를 통해 경사하강법으로 해결했다는 점이 가장 큰 특징입니다.

결과적으로 이 논문은 기존에 SOTA를 찍었던 강화학습 혹은 진화 알고리즘 기반 논문들과 비교해 보았을 때, 비슷하거나 더 높은 성능을 보이면서도 훨씬 빠르게 수렴할 수 있었습니다.

이것은 바이레벨 옵티마이제이션을 기반으로 근사하는 방식을 이용합니다.
이게 어떻게 이루어지는지 살펴보도록 하겠습니다.

먼저 이 논문은 서치 스페이스를 DAG로 정의합니다.
이것은 residual network를 고려하기 때문입니다.
서치 스페이스에 주어진 노드가 4개라고 가정한다고 봤을때,
각 노드는 엣지의 인풋 혹은 아웃풋이 됩니다.
여기서 엣지는 연산자입니다.
아키텍처 서치로 이 연산자가 무엇이 될 지 찾는데,
이 논문의 특징은 이 연산자들을 가중합으로 둔다는 것입니다.
따라서 가능한 모든 연산자들에는 가중치가 각각 주어지고,
이 가중치들은 소프트맥스를 통해 각자의 가중치가 정해지게 됩니다.
이 중에 높은 소프트맥스 값을 가지는 연산자가 o와 mixed o 에서 차지하는 비중이 높아지고, 학습이 끝나면 가장 높은 값을 가지는 연산자가 선택됩니다. 



